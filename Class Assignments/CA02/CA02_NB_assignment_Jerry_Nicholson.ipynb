{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a9ffb66",
      "metadata": {
        "id": "9a9ffb66"
      },
      "source": [
        "CA02: This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm.\n",
        "\n",
        "Jerry and Nicholson\n",
        "\n",
        "Assignmetn Summary:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c47b5690",
      "metadata": {
        "id": "c47b5690"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Purpose:\n",
        "\n",
        "These imports provide file system access, word counting utilities, numerical arrays, and the Naive Bayes model with an accuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBwjdTg6VC3L",
        "outputId": "ace4c77f-a8ea-47bc-94a4-1959607eab7f"
      },
      "id": "IBwjdTg6VC3L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/CA02-Jerry and Nichlison\" /content/\n"
      ],
      "metadata": {
        "id": "sKsKWW3tVPNW"
      },
      "id": "sKsKWW3tVPNW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf_T8rg2Vi0w",
        "outputId": "a0854374-83bb-484a-c960-bc36ef5519e5"
      },
      "id": "Sf_T8rg2Vi0w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'CA02-Jerry and Nichlison'   drive   sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd4f298",
      "metadata": {
        "id": "edd4f298"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cbf933f",
      "metadata": {
        "id": "5cbf933f"
      },
      "source": [
        "# Part1 Building the Dictionary\n",
        "\n",
        "Purpose:\n",
        "\n",
        "This part of the code is responsible for looking at ALL training emails and deciding which words are important enough to become features for the Naive Bayes model.\n",
        "\n",
        "The goal is to build a dictionary of words that represents the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4b41db",
      "metadata": {
        "id": "0e4b41db"
      },
      "outputs": [],
      "source": [
        "def make_dictionary(root_dir, vocab_size=3000):\n",
        "    # Collect all words from all training emails\n",
        "    all_words = []\n",
        "    emails = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
        "\n",
        "    for mail in emails:\n",
        "        if not os.path.isfile(mail):\n",
        "            continue\n",
        "        # Use latin-1 to safely read the legacy email text files\n",
        "        with open(mail, \"r\", encoding=\"latin-1\") as m:\n",
        "            for line in m:\n",
        "                words = line.lower().split()\n",
        "                all_words.extend(words)\n",
        "\n",
        "    # Count frequency of every word\n",
        "    dictionary = Counter(all_words)\n",
        "\n",
        "    # Remove tokens that are not useful features\n",
        "    for item in list(dictionary):\n",
        "        if (not item.isalpha()) or len(item) == 1:\n",
        "            del dictionary[item]\n",
        "\n",
        "    # Keep the top N most common words\n",
        "    return dictionary.most_common(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d491d70",
      "metadata": {
        "id": "3d491d70"
      },
      "source": [
        "## Breakdown of the code logic:\n",
        "\n",
        "#1 Collect words from every training email.\n",
        "\n",
        "We list all files in the training folder, skip non-files, open each email, lowercase the text, and append every word into a single list. Lowercasing ensures \"Free\" and \"free\" are treated as the same token.\n",
        "\n",
        "#2 Count word frequency across the corpus.\n",
        "\n",
        "`Counter(all_words)` computes how many times each word appears in the training set so we can rank words by importance.\n",
        "\n",
        "#3 Clean the vocabulary.\n",
        "\n",
        "We remove any token that is not purely alphabetic and any one-letter token, which filters out punctuation, numbers, and noisy tokens.\n",
        "\n",
        "#4 Limit the vocabulary size.\n",
        "\n",
        "We keep only the top `vocab_size` most frequent words, which reduces noise and keeps the feature matrix manageable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb205ee",
      "metadata": {
        "id": "7bb205ee"
      },
      "source": [
        "# Part2 Extract features\n",
        "\n",
        "Purpose:\n",
        "\n",
        "This part is to transform each email from text file into a numerical feature vector based on word frequencies and assign a spam or non-spam label using the file naming rule, so the data becomes usable for Naive Bayes classification.\n",
        "\n",
        "The goal to create two outputs, feature matrix and label array, these two outputs are the input required to train and test the Naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2fda2d",
      "metadata": {
        "id": "fc2fda2d"
      },
      "outputs": [],
      "source": [
        "def extract_features(mail_dir, dictionary):\n",
        "    # Build file list and allocate output arrays\n",
        "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
        "    vocab_size = len(dictionary)\n",
        "    features_matrix = np.zeros((len(files), vocab_size), dtype=np.int32)\n",
        "    train_labels = np.zeros(len(files), dtype=np.int32)\n",
        "\n",
        "    # Map each dictionary word to its column index\n",
        "    word_index = {word: i for i, (word, _) in enumerate(dictionary)}\n",
        "\n",
        "    for doc_id, fil in enumerate(files):\n",
        "        if not os.path.isfile(fil):\n",
        "            continue\n",
        "\n",
        "        # Count word occurrences in the whole email\n",
        "        with open(fil, \"r\", encoding=\"latin-1\") as fi:\n",
        "            word_counts = Counter()\n",
        "            for line in fi:\n",
        "                words = line.lower().split()\n",
        "                word_counts.update(words)\n",
        "\n",
        "        # Fill the feature vector for this email\n",
        "        for word, count in word_counts.items():\n",
        "            idx = word_index.get(word)\n",
        "            if idx is not None:\n",
        "                features_matrix[doc_id, idx] = count\n",
        "\n",
        "        # Label spam based on filename convention\n",
        "        filename = os.path.basename(fil)\n",
        "        train_labels[doc_id] = 1 if filename.startswith(\"spmsg\") else 0\n",
        "\n",
        "    return features_matrix, train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4cc56a",
      "metadata": {
        "id": "ae4cc56a"
      },
      "source": [
        "## Breakdown of the code logic:\n",
        "\n",
        "#1 Initialize arrays for features and labels.\n",
        "\n",
        "We create a feature matrix with one row per email and one column per dictionary word, plus a label vector to store spam (1) or non-spam (0).\n",
        "\n",
        "#2 Build a fast lookup table.\n",
        "\n",
        "`word_index` maps each dictionary word to its column index so we can update the feature matrix in O(1) time per word.\n",
        "\n",
        "#3 Read each email and count words.\n",
        "\n",
        "For each file, we read all lines, lowercase the text, and use `Counter` to count how many times each word appears in that email.\n",
        "\n",
        "#4 Fill the feature matrix.\n",
        "\n",
        "Only words that exist in the dictionary are written into the feature matrix, keeping the columns consistent across train and test.\n",
        "\n",
        "#5 Assign labels based on filenames.\n",
        "\n",
        "Emails whose filenames start with `spmsg` are labeled as spam (1); all others are labeled non-spam (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f51ca029",
      "metadata": {
        "id": "f51ca029"
      },
      "source": [
        "# Data Paths\n",
        "\n",
        "Purpose:\n",
        "\n",
        "This cell sets the directory paths for the training and testing email folders. Adjust these paths if your folder structure is different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5070e7fd",
      "metadata": {
        "id": "5070e7fd"
      },
      "outputs": [],
      "source": [
        "# Enter the paths of the training and testing folders\n",
        "# Update these if your data is stored elsewhere\n",
        "TRAIN_DIR = \"./test-mails\"\n",
        "TEST_DIR = \"./train-mails\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TRAIN_DIR =\", TRAIN_DIR)\n",
        "print(\"TEST_DIR  =\", TEST_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADxLffIXWGeD",
        "outputId": "e989deae-a6b9-46c5-bb06-b55270843cdc"
      },
      "id": "ADxLffIXWGeD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_DIR = ./test-mails\n",
            "TEST_DIR  = ./train-mails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/CA02-Jerry and Nichlison\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaJU1svOWScA",
        "outputId": "df716641-171d-4558-8593-6598e04f99da"
      },
      "id": "LaJU1svOWScA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CA02_NB_assignment_Jerry_Nicholson.ipynb  __MACOSX  test-mails\ttrain-mails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/CA02-Jerry and Nichlison\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuv0cSHvWZmx",
        "outputId": "8ed36be9-7406-4119-c0f4-0b0288b5bb1d"
      },
      "id": "nuv0cSHvWZmx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CA02-Jerry and Nichlison\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = make_dictionary(TRAIN_DIR)\n"
      ],
      "metadata": {
        "id": "eZKHYBVUWkDF"
      },
      "id": "eZKHYBVUWkDF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = \"./train-mails\"\n",
        "TEST_DIR  = \"./test-mails\"\n",
        "\n",
        "print(\"TRAIN_DIR =\", TRAIN_DIR)\n",
        "print(\"TEST_DIR  =\", TEST_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqjR21ukWzDn",
        "outputId": "d777bbbf-1cf7-498f-a66c-78ab14e87c64"
      },
      "id": "hqjR21ukWzDn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_DIR = ./train-mails\n",
            "TEST_DIR  = ./test-mails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"./train-mails\"\n",
        "!ls \"./test-mails\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxCfUfZ3W1Zc",
        "outputId": "3496bcfb-2c39-43bd-bc25-f2be98827eae"
      },
      "id": "MxCfUfZ3W1Zc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-1msg1.txt\t6-21msg2.txt   6-832msg1.txt  spmsga166.txt  spmsgb143.txt\n",
            "3-1msg2.txt\t6-21msg3.txt   6-833msg1.txt  spmsga16.txt   spmsgb144.txt\n",
            "3-1msg3.txt\t6-22msg1.txt   6-89msg1.txt   spmsga17.txt   spmsgb145.txt\n",
            "3-375msg1.txt\t6-241msg2.txt  6-93msg1.txt   spmsga18.txt   spmsgb146.txt\n",
            "3-378msg1.txt\t6-241msg3.txt  6-96msg1.txt   spmsga19.txt   spmsgb147.txt\n",
            "3-378msg2.txt\t6-243msg1.txt  6-97msg1.txt   spmsga1.txt    spmsgb148.txt\n",
            "3-378msg3.txt\t6-245msg1.txt  8-809msg1.txt  spmsga20.txt   spmsgb149.txt\n",
            "3-378msg4.txt\t6-245msg2.txt  8-811msg1.txt  spmsga21.txt   spmsgb14.txt\n",
            "3-378msg5.txt\t6-245msg3.txt  8-811msg2.txt  spmsga22.txt   spmsgb150.txt\n",
            "3-379msg1.txt\t6-248msg1.txt  8-814msg1.txt  spmsga23.txt   spmsgb151.txt\n",
            "3-379msg2.txt\t6-248msg2.txt  8-815msg1.txt  spmsga24.txt   spmsgb152.txt\n",
            "3-379msg3.txt\t6-248msg3.txt  8-817msg1.txt  spmsga25.txt   spmsgb153.txt\n",
            "3-380msg1.txt\t6-249msg1.txt  8-817msg2.txt  spmsga26.txt   spmsgb154.txt\n",
            "3-380msg2.txt\t6-250msg1.txt  8-817msg3.txt  spmsga27.txt   spmsgb155.txt\n",
            "3-380msg3.txt\t6-251msg1.txt  8-820msg1.txt  spmsga28.txt   spmsgb156.txt\n",
            "3-380msg4.txt\t6-252msg1.txt  8-821msg1.txt  spmsga29.txt   spmsgb157.txt\n",
            "3-380msg5.txt\t6-254msg1.txt  8-824msg1.txt  spmsga2.txt    spmsgb158.txt\n",
            "3-380msg6.txt\t6-255msg1.txt  8-826msg1.txt  spmsga30.txt   spmsgb159.txt\n",
            "3-380msg7.txt\t6-255msg2.txt  8-828msg1.txt  spmsga31.txt   spmsgb15.txt\n",
            "3-383msg0.txt\t6-255msg3.txt  8-828msg2.txt  spmsga32.txt   spmsgb160.txt\n",
            "3-383msg1.txt\t6-255msg4.txt  8-828msg3.txt  spmsga33.txt   spmsgb161.txt\n",
            "3-384msg0.txt\t6-255msg5.txt  8-828msg4.txt  spmsga34.txt   spmsgb162.txt\n",
            "3-384msg1.txt\t6-256msg1.txt  8-829msg1.txt  spmsga35.txt   spmsgb163.txt\n",
            "3-384msg2.txt\t6-257msg1.txt  8-833msg1.txt  spmsga36.txt   spmsgb164.txt\n",
            "3-384msg3.txt\t6-266msg1.txt  8-835msg1.txt  spmsga37.txt   spmsgb165.txt\n",
            "3-385msg1.txt\t6-266msg2.txt  8-836msg1.txt  spmsga38.txt   spmsgb166.txt\n",
            "3-385msg2.txt\t6-266msg3.txt  8-837msg1.txt  spmsga39.txt   spmsgb167.txt\n",
            "3-385msg3.txt\t6-267msg1.txt  8-837msg2.txt  spmsga3.txt    spmsgb168.txt\n",
            "3-387msg0.txt\t6-268msg1.txt  8-838msg1.txt  spmsga40.txt   spmsgb16.txt\n",
            "3-387msg1.txt\t6-270msg1.txt  8-838msg2.txt  spmsga41.txt   spmsgb17.txt\n",
            "3-387msg2.txt\t6-271msg1.txt  8-840msg1.txt  spmsga42.txt   spmsgb18.txt\n",
            "3-388msg1.txt\t6-272msg1.txt  8-842msg1.txt  spmsga43.txt   spmsgb19.txt\n",
            "3-389msg1.txt\t6-273msg1.txt  8-844msg1.txt  spmsga44.txt   spmsgb1.txt\n",
            "3-390msg0.txt\t6-273msg2.txt  8-845msg1.txt  spmsga45.txt   spmsgb20.txt\n",
            "3-390msg1.txt\t6-273msg3.txt  8-846msg1.txt  spmsga46.txt   spmsgb21.txt\n",
            "3-390msg2.txt\t6-281msg1.txt  8-858msg1.txt  spmsga47.txt   spmsgb22.txt\n",
            "3-390msg3.txt\t6-28msg1.txt   8-859msg1.txt  spmsga48.txt   spmsgb23.txt\n",
            "3-390msg4.txt\t6-292msg1.txt  8-861msg1.txt  spmsga49.txt   spmsgb24.txt\n",
            "3-390msg5.txt\t6-293msg1.txt  8-862msg1.txt  spmsga4.txt    spmsgb25.txt\n",
            "3-391msg1.txt\t6-31msg1.txt   8-863msg1.txt  spmsga50.txt   spmsgb26.txt\n",
            "3-392msg0.txt\t6-338msg1.txt  8-863msg2.txt  spmsga51.txt   spmsgb27.txt\n",
            "3-392msg1.txt\t6-33msg1.txt   8-863msg3.txt  spmsga52.txt   spmsgb28.txt\n",
            "3-395msg1.txt\t6-36msg1.txt   8-864msg1.txt  spmsga53.txt   spmsgb29.txt\n",
            "3-395msg2.txt\t6-380msg1.txt  8-865msg1.txt  spmsga54.txt   spmsgb2.txt\n",
            "3-397msg1.txt\t6-381msg1.txt  8-865msg2.txt  spmsga55.txt   spmsgb30.txt\n",
            "3-398msg1.txt\t6-381msg2.txt  8-865msg3.txt  spmsga56.txt   spmsgb31.txt\n",
            "3-401msg1.txt\t6-381msg3.txt  8-867msg1.txt  spmsga57.txt   spmsgb32.txt\n",
            "3-402msg1.txt\t6-381msg4.txt  8-871msg1.txt  spmsga58.txt   spmsgb33.txt\n",
            "5-1298msg1.txt\t6-381msg5.txt  8-872msg1.txt  spmsga59.txt   spmsgb34.txt\n",
            "5-1298msg2.txt\t6-382msg1.txt  8-873msg1.txt  spmsga5.txt    spmsgb35.txt\n",
            "5-1298msg3.txt\t6-392msg1.txt  8-874msg1.txt  spmsga60.txt   spmsgb36.txt\n",
            "5-1300msg1.txt\t6-41msg1.txt   8-875msg1.txt  spmsga61.txt   spmsgb37.txt\n",
            "5-1300msg2.txt\t6-421msg1.txt  8-876msg1.txt  spmsga62.txt   spmsgb38.txt\n",
            "5-1300msg3.txt\t6-425msg2.txt  8-876msg2.txt  spmsga63.txt   spmsgb39.txt\n",
            "5-1301msg1.txt\t6-425msg3.txt  8-878msg1.txt  spmsga64.txt   spmsgb3.txt\n",
            "5-1302msg1.txt\t6-42msg1.txt   8-879msg1.txt  spmsga65.txt   spmsgb40.txt\n",
            "5-1303msg1.txt\t6-42msg2.txt   8-881msg1.txt  spmsga66.txt   spmsgb41.txt\n",
            "5-1303msg2.txt\t6-42msg3.txt   8-884msg1.txt  spmsga67.txt   spmsgb42.txt\n",
            "5-1303msg3.txt\t6-430msg1.txt  8-885msg1.txt  spmsga68.txt   spmsgb43.txt\n",
            "5-1304msg1.txt\t6-430msg2.txt  8-887msg1.txt  spmsga69.txt   spmsgb44.txt\n",
            "5-1307msg1.txt\t6-430msg3.txt  8-887msg2.txt  spmsga6.txt    spmsgb45.txt\n",
            "5-1307msg2.txt\t6-430msg4.txt  8-890msg1.txt  spmsga70.txt   spmsgb46.txt\n",
            "5-1307msg3.txt\t6-430msg5.txt  8-892msg1.txt  spmsga71.txt   spmsgb47.txt\n",
            "5-1311msg1.txt\t6-453msg0.txt  8-893msg1.txt  spmsga72.txt   spmsgb48.txt\n",
            "5-1311msg2.txt\t6-453msg1.txt  8-894msg1.txt  spmsga73.txt   spmsgb49.txt\n",
            "5-1311msg3.txt\t6-453msg2.txt  8-895msg1.txt  spmsga74.txt   spmsgb4.txt\n",
            "5-1312msg1.txt\t6-454msg1.txt  8-896msg1.txt  spmsga75.txt   spmsgb50.txt\n",
            "5-1312msg2.txt\t6-455msg1.txt  8-897msg1.txt  spmsga76.txt   spmsgb51.txt\n",
            "5-1312msg3.txt\t6-455msg2.txt  8-898msg1.txt  spmsga77.txt   spmsgb52.txt\n",
            "5-1315msg1.txt\t6-45msg1.txt   spmsga100.txt  spmsga78.txt   spmsgb53.txt\n",
            "5-1315msg2.txt\t6-473msg1.txt  spmsga101.txt  spmsga79.txt   spmsgb54.txt\n",
            "5-1315msg3.txt\t6-474msg1.txt  spmsga102.txt  spmsga7.txt    spmsgb55.txt\n",
            "5-1315msg4.txt\t6-4msg1.txt    spmsga103.txt  spmsga80.txt   spmsgb56.txt\n",
            "5-1315msg5.txt\t6-4msg2.txt    spmsga104.txt  spmsga81.txt   spmsgb57.txt\n",
            "5-1316msg1.txt\t6-4msg3.txt    spmsga105.txt  spmsga82.txt   spmsgb58.txt\n",
            "5-1318msg1.txt\t6-50msg0.txt   spmsga106.txt  spmsga83.txt   spmsgb59.txt\n",
            "5-1318msg2.txt\t6-50msg1.txt   spmsga107.txt  spmsga84.txt   spmsgb5.txt\n",
            "5-1318msg3.txt\t6-50msg2.txt   spmsga108.txt  spmsga85.txt   spmsgb60.txt\n",
            "5-1321msg1.txt\t6-50msg3.txt   spmsga109.txt  spmsga86.txt   spmsgb61.txt\n",
            "5-1322msg1.txt\t6-51msg1.txt   spmsga10.txt   spmsga87.txt   spmsgb62.txt\n",
            "5-1324msg1.txt\t6-52msg1.txt   spmsga110.txt  spmsga88.txt   spmsgb63.txt\n",
            "5-1325msg1.txt\t6-53msg1.txt   spmsga111.txt  spmsga89.txt   spmsgb64.txt\n",
            "5-1326msg1.txt\t6-54msg1.txt   spmsga112.txt  spmsga8.txt    spmsgb65.txt\n",
            "5-1327msg1.txt\t6-55msg1.txt   spmsga113.txt  spmsga90.txt   spmsgb66.txt\n",
            "5-1328msg1.txt\t6-57msg1.txt   spmsga114.txt  spmsga91.txt   spmsgb67.txt\n",
            "5-1328msg2.txt\t6-60msg1.txt   spmsga115.txt  spmsga92.txt   spmsgb68.txt\n",
            "5-1328msg3.txt\t6-61msg1.txt   spmsga116.txt  spmsga93.txt   spmsgb69.txt\n",
            "5-1329msg1.txt\t6-64msg1.txt   spmsga117.txt  spmsga94.txt   spmsgb6.txt\n",
            "5-1330msg1.txt\t6-65msg1.txt   spmsga118.txt  spmsga95.txt   spmsgb70.txt\n",
            "5-1331msg1.txt\t6-68msg1.txt   spmsga119.txt  spmsga96.txt   spmsgb71.txt\n",
            "5-1332msg1.txt\t6-70msg1.txt   spmsga11.txt   spmsga97.txt   spmsgb72.txt\n",
            "5-1333msg1.txt\t6-72msg1.txt   spmsga120.txt  spmsga98.txt   spmsgb73.txt\n",
            "5-1335msg1.txt\t6-73msg1.txt   spmsga121.txt  spmsga99.txt   spmsgb74.txt\n",
            "5-1337msg1.txt\t6-74msg1.txt   spmsga122.txt  spmsga9.txt    spmsgb75.txt\n",
            "5-1338msg1.txt\t6-75msg1.txt   spmsga123.txt  spmsgb100.txt  spmsgb76.txt\n",
            "5-1339msg1.txt\t6-76msg1.txt   spmsga124.txt  spmsgb101.txt  spmsgb77.txt\n",
            "6-107msg1.txt\t6-77msg1.txt   spmsga125.txt  spmsgb102.txt  spmsgb78.txt\n",
            "6-107msg2.txt\t6-798msg3.txt  spmsga126.txt  spmsgb103.txt  spmsgb79.txt\n",
            "6-107msg3.txt\t6-799msg1.txt  spmsga127.txt  spmsgb104.txt  spmsgb7.txt\n",
            "6-108msg1.txt\t6-7msg1.txt    spmsga128.txt  spmsgb105.txt  spmsgb80.txt\n",
            "6-109msg1.txt\t6-7msg2.txt    spmsga129.txt  spmsgb106.txt  spmsgb81.txt\n",
            "6-10msg1.txt\t6-7msg3.txt    spmsga12.txt   spmsgb107.txt  spmsgb82.txt\n",
            "6-10msg2.txt\t6-801msg1.txt  spmsga130.txt  spmsgb108.txt  spmsgb83.txt\n",
            "6-10msg3.txt\t6-801msg2.txt  spmsga131.txt  spmsgb109.txt  spmsgb84.txt\n",
            "6-110msg1.txt\t6-801msg3.txt  spmsga132.txt  spmsgb10.txt   spmsgb85.txt\n",
            "6-110msg2.txt\t6-802msg0.txt  spmsga133.txt  spmsgb110.txt  spmsgb86.txt\n",
            "6-110msg3.txt\t6-802msg1.txt  spmsga134.txt  spmsgb111.txt  spmsgb87.txt\n",
            "6-112msg1.txt\t6-802msg2.txt  spmsga135.txt  spmsgb112.txt  spmsgb88.txt\n",
            "6-113msg1.txt\t6-806msg1.txt  spmsga136.txt  spmsgb113.txt  spmsgb89.txt\n",
            "6-113msg2.txt\t6-806msg2.txt  spmsga137.txt  spmsgb114.txt  spmsgb8.txt\n",
            "6-113msg3.txt\t6-806msg3.txt  spmsga138.txt  spmsgb115.txt  spmsgb90.txt\n",
            "6-118msg1.txt\t6-809msg1.txt  spmsga139.txt  spmsgb116.txt  spmsgb91.txt\n",
            "6-119msg1.txt\t6-809msg2.txt  spmsga13.txt   spmsgb117.txt  spmsgb92.txt\n",
            "6-11msg1.txt\t6-809msg3.txt  spmsga140.txt  spmsgb118.txt  spmsgb93.txt\n",
            "6-120msg1.txt\t6-80msg1.txt   spmsga141.txt  spmsgb119.txt  spmsgb94.txt\n",
            "6-121msg1.txt\t6-80msg2.txt   spmsga142.txt  spmsgb11.txt   spmsgb95.txt\n",
            "6-122msg1.txt\t6-80msg3.txt   spmsga143.txt  spmsgb120.txt  spmsgb96.txt\n",
            "6-124msg1.txt\t6-80msg4.txt   spmsga144.txt  spmsgb121.txt  spmsgb97.txt\n",
            "6-125msg1.txt\t6-80msg5.txt   spmsga145.txt  spmsgb122.txt  spmsgb98.txt\n",
            "6-126msg1.txt\t6-813msg1.txt  spmsga146.txt  spmsgb123.txt  spmsgb99.txt\n",
            "6-132msg1.txt\t6-813msg2.txt  spmsga147.txt  spmsgb124.txt  spmsgb9.txt\n",
            "6-134msg1.txt\t6-813msg3.txt  spmsga148.txt  spmsgb125.txt  spmsgc10.txt\n",
            "6-139msg1.txt\t6-816msg1.txt  spmsga149.txt  spmsgb126.txt  spmsgc11.txt\n",
            "6-141msg1.txt\t6-818msg1.txt  spmsga14.txt   spmsgb127.txt  spmsgc12.txt\n",
            "6-146msg1.txt\t6-819msg1.txt  spmsga150.txt  spmsgb128.txt  spmsgc13.txt\n",
            "6-14msg1.txt\t6-81msg1.txt   spmsga151.txt  spmsgb129.txt  spmsgc14.txt\n",
            "6-14msg2.txt\t6-820msg1.txt  spmsga152.txt  spmsgb12.txt   spmsgc15.txt\n",
            "6-14msg3.txt\t6-823msg1.txt  spmsga153.txt  spmsgb130.txt  spmsgc16.txt\n",
            "6-150msg1.txt\t6-823msg2.txt  spmsga154.txt  spmsgb131.txt  spmsgc17.txt\n",
            "6-150msg2.txt\t6-825msg1.txt  spmsga155.txt  spmsgb132.txt  spmsgc1.txt\n",
            "6-150msg3.txt\t6-825msg2.txt  spmsga156.txt  spmsgb133.txt  spmsgc2.txt\n",
            "6-151msg1.txt\t6-825msg3.txt  spmsga157.txt  spmsgb134.txt  spmsgc3.txt\n",
            "6-151msg2.txt\t6-828msg1.txt  spmsga158.txt  spmsgb135.txt  spmsgc4.txt\n",
            "6-151msg3.txt\t6-828msg2.txt  spmsga159.txt  spmsgb136.txt  spmsgc5.txt\n",
            "6-15msg1.txt\t6-828msg3.txt  spmsga15.txt   spmsgb137.txt  spmsgc6.txt\n",
            "6-15msg2.txt\t6-829msg1.txt  spmsga160.txt  spmsgb138.txt  spmsgc7.txt\n",
            "6-15msg3.txt\t6-829msg2.txt  spmsga161.txt  spmsgb139.txt  spmsgc8.txt\n",
            "6-16msg1.txt\t6-829msg3.txt  spmsga162.txt  spmsgb13.txt   spmsgc9.txt\n",
            "6-1msg1.txt\t6-82msg1.txt   spmsga163.txt  spmsgb140.txt\n",
            "6-20msg1.txt\t6-830msg1.txt  spmsga164.txt  spmsgb141.txt\n",
            "6-21msg1.txt\t6-831msg1.txt  spmsga165.txt  spmsgb142.txt\n",
            "8-899msg1.txt\t9-1294msg1.txt\t9-628msg1.txt  spmsgc126.txt  spmsgc48.txt\n",
            "8-900msg1.txt\t9-1295msg1.txt\t9-62msg1.txt   spmsgc127.txt  spmsgc49.txt\n",
            "8-901msg1.txt\t9-1296msg1.txt\t9-63msg1.txt   spmsgc128.txt  spmsgc50.txt\n",
            "8-905msg1.txt\t9-133msg1.txt\t9-64msg1.txt   spmsgc129.txt  spmsgc51.txt\n",
            "8-905msg2.txt\t9-133msg2.txt\t9-64msg2.txt   spmsgc130.txt  spmsgc52.txt\n",
            "8-907msg1.txt\t9-138msg1.txt\t9-66msg1.txt   spmsgc131.txt  spmsgc53.txt\n",
            "8-912msg1.txt\t9-138msg2.txt\t9-6msg1.txt    spmsgc132.txt  spmsgc54.txt\n",
            "8-916msg1.txt\t9-140msg1.txt\t9-6msg2.txt    spmsgc133.txt  spmsgc55.txt\n",
            "8-922msg1.txt\t9-142msg1.txt\t9-71msg1.txt   spmsgc134.txt  spmsgc56.txt\n",
            "8-922msg2.txt\t9-142msg2.txt\t9-72msg1.txt   spmsgc135.txt  spmsgc57.txt\n",
            "8-922msg3.txt\t9-146msg1.txt\t9-72msg2.txt   spmsgc136.txt  spmsgc58.txt\n",
            "8-923msg1.txt\t9-147msg1.txt\t9-74msg1.txt   spmsgc137.txt  spmsgc59.txt\n",
            "8-925msg1.txt\t9-147msg2.txt\t9-74msg2.txt   spmsgc138.txt  spmsgc60.txt\n",
            "8-930msg1.txt\t9-148msg1.txt\t9-78msg1.txt   spmsgc139.txt  spmsgc61.txt\n",
            "8-931msg1.txt\t9-14msg1.txt\t9-78msg2.txt   spmsgc140.txt  spmsgc62.txt\n",
            "8-933msg1.txt\t9-150msg1.txt\t9-79msg1.txt   spmsgc141.txt  spmsgc63.txt\n",
            "8-933msg2.txt\t9-152msg1.txt\t9-83msg1.txt   spmsgc142.txt  spmsgc64.txt\n",
            "8-933msg3.txt\t9-152msg2.txt\t9-83msg2.txt   spmsgc143.txt  spmsgc65.txt\n",
            "8-936msg1.txt\t9-153msg1.txt\t9-86msg1.txt   spmsgc144.txt  spmsgc66.txt\n",
            "8-937msg1.txt\t9-154msg1.txt\t9-86msg2.txt   spmsgc145.txt  spmsgc67.txt\n",
            "8-949msg1.txt\t9-154msg2.txt\t9-87msg1.txt   spmsgc146.txt  spmsgc68.txt\n",
            "8-949msg2.txt\t9-157msg1.txt\t9-88msg1.txt   spmsgc147.txt  spmsgc69.txt\n",
            "8-951msg1.txt\t9-157msg2.txt\t9-88msg2.txt   spmsgc18.txt   spmsgc70.txt\n",
            "8-956msg1.txt\t9-159msg1.txt\t9-94msg1.txt   spmsgc19.txt   spmsgc71.txt\n",
            "8-963msg1.txt\t9-159msg2.txt\t9-94msg2.txt   spmsgc20.txt   spmsgc72.txt\n",
            "8-963msg2.txt\t9-15msg1.txt\t9-98msg1.txt   spmsgc21.txt   spmsgc73.txt\n",
            "8-965msg1.txt\t9-15msg2.txt\tspmsgc100.txt  spmsgc22.txt   spmsgc74.txt\n",
            "8-965msg2.txt\t9-162msg1.txt\tspmsgc101.txt  spmsgc23.txt   spmsgc75.txt\n",
            "8-972msg1.txt\t9-167msg1.txt\tspmsgc102.txt  spmsgc24.txt   spmsgc76.txt\n",
            "8-972msg2.txt\t9-168msg1.txt\tspmsgc103.txt  spmsgc25.txt   spmsgc77.txt\n",
            "8-984msg1.txt\t9-597msg1.txt\tspmsgc104.txt  spmsgc26.txt   spmsgc78.txt\n",
            "8-987msg1.txt\t9-597msg2.txt\tspmsgc105.txt  spmsgc27.txt   spmsgc79.txt\n",
            "8-987msg2.txt\t9-59msg1.txt\tspmsgc106.txt  spmsgc28.txt   spmsgc80.txt\n",
            "9-1262msg1.txt\t9-5msg1.txt\tspmsgc107.txt  spmsgc29.txt   spmsgc81.txt\n",
            "9-1263msg1.txt\t9-5msg2.txt\tspmsgc108.txt  spmsgc30.txt   spmsgc82.txt\n",
            "9-1264msg1.txt\t9-606msg1.txt\tspmsgc109.txt  spmsgc31.txt   spmsgc83.txt\n",
            "9-1266msg1.txt\t9-607msg1.txt\tspmsgc110.txt  spmsgc32.txt   spmsgc84.txt\n",
            "9-1266msg2.txt\t9-607msg2.txt\tspmsgc111.txt  spmsgc33.txt   spmsgc85.txt\n",
            "9-1267msg1.txt\t9-608msg1.txt\tspmsgc112.txt  spmsgc34.txt   spmsgc86.txt\n",
            "9-1267msg2.txt\t9-608msg2.txt\tspmsgc113.txt  spmsgc35.txt   spmsgc87.txt\n",
            "9-126msg1.txt\t9-612msg1.txt\tspmsgc114.txt  spmsgc36.txt   spmsgc88.txt\n",
            "9-126msg2.txt\t9-612msg2.txt\tspmsgc115.txt  spmsgc37.txt   spmsgc89.txt\n",
            "9-1273msg1.txt\t9-613msg1.txt\tspmsgc116.txt  spmsgc38.txt   spmsgc90.txt\n",
            "9-1274msg1.txt\t9-616msg1.txt\tspmsgc117.txt  spmsgc39.txt   spmsgc91.txt\n",
            "9-1275msg1.txt\t9-617msg1.txt\tspmsgc118.txt  spmsgc40.txt   spmsgc92.txt\n",
            "9-1276msg1.txt\t9-619msg1.txt\tspmsgc119.txt  spmsgc41.txt   spmsgc93.txt\n",
            "9-1288msg1.txt\t9-61msg1.txt\tspmsgc120.txt  spmsgc42.txt   spmsgc94.txt\n",
            "9-1289msg1.txt\t9-621msg1.txt\tspmsgc121.txt  spmsgc43.txt   spmsgc95.txt\n",
            "9-128msg1.txt\t9-621msg2.txt\tspmsgc122.txt  spmsgc44.txt   spmsgc96.txt\n",
            "9-1290msg1.txt\t9-625msg1.txt\tspmsgc123.txt  spmsgc45.txt   spmsgc97.txt\n",
            "9-1292msg1.txt\t9-627msg1.txt\tspmsgc124.txt  spmsgc46.txt   spmsgc98.txt\n",
            "9-1293msg1.txt\t9-627msg2.txt\tspmsgc125.txt  spmsgc47.txt   spmsgc99.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = make_dictionary(TRAIN_DIR)\n"
      ],
      "metadata": {
        "id": "RlVE4RcuW4S2"
      },
      "id": "RlVE4RcuW4S2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1995ee8f",
      "metadata": {
        "id": "1995ee8f"
      },
      "source": [
        "# Build Feature Matrices\n",
        "\n",
        "Purpose:\n",
        "\n",
        "This cell builds the vocabulary using only the training data, then converts both the training and test emails into numeric feature matrices that share the same column order.\n",
        "\n",
        "Key idea:\n",
        "\n",
        "The dictionary must be created from training data only to avoid data leakage, and the same dictionary is used for both train and test so the model sees consistent features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c726beba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c726beba",
        "outputId": "d703fd11-032b-46b2-c2f2-5f3dc521044a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n"
          ]
        }
      ],
      "source": [
        "# Build the dictionary from training emails only\n",
        "dictionary = make_dictionary(TRAIN_DIR)\n",
        "\n",
        "# Convert emails to feature vectors using the same dictionary\n",
        "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR, dictionary)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR, dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798a907c",
      "metadata": {
        "id": "798a907c"
      },
      "source": [
        "# Train and Evaluate the Model\n",
        "\n",
        "Purpose:\n",
        "\n",
        "This cell trains a Multinomial Naive Bayes classifier on the training feature matrix, predicts labels for the test set, and prints the accuracy score.\n",
        "\n",
        "Why Multinomial Naive Bayes:\n",
        "\n",
        "The features are word counts, so MultinomialNB is a natural choice because it models counts directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96527a4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96527a4b",
        "outputId": "76a9cb36-363e-4379-de6d-9f4909a1d711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model using Multinomial Naive Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Model using Multinomial Naive Bayes algorithm .....\")\n",
        "# Create and fit the model on training data\n",
        "model = MultinomialNB()\n",
        "model.fit(features_matrix, labels)\n",
        "print(\"Training completed\")\n",
        "\n",
        "# Predict test labels and report accuracy\n",
        "print(\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print(\n",
        "    \"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\"\n",
        ")\n",
        "print(accuracy_score(test_labels, predicted_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c638a7b",
      "metadata": {
        "id": "9c638a7b"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}